The simplest tokeniser:

1. Splits text into chunks in some rule-based way, e.g. a word-based tokeniser would split on sentences and punctuation.

```python
text = "Hello, world!"
tokens = text.split()
print(tokens)
```

```
['Hello,', 'world!']
```

Python's `split()` method only splits on spaces by default. We can use regular expressions to cover spaces and punctuation that also delimit words.

<details>
<summary>How does the `re.findall(r'\w+|[^\w\s]')` regex work?</summary>

It has two parts separated by `|` (which means "or"):

- `\w+` matches one or more word characters (letters, digits, underscores).
- `[^\w\s]` matches anything that's not a word character and not whitespace. So it matches punctuation.

The `^` inside the brackets `[^...]` means "not", so `[^\w\s]` literally means "not a word character, not whitespace".

---

Both `split()` and `findall()` break text into pieces, but they work differently:

- `split()`: You tell it what to remove (the separators), and it gives you what's left.
- `findall()`: You tell it what to keep (the pattern), and it gives you all matches.

</details>

```python
import re
tokens = re.findall(r'\w+|[^\w\s]', text)
print(tokens)
```

```
['Hello', ',', 'world', '!']
```

2. Maps those chunks to numeric values:

```python
vocab = {token: idx for idx, token in enumerate(set(tokens))}
print(vocab)
```

```
{'Hello': 0, ',': 1, '!': 2, 'world': 3}
```

```python
token_ids = [vocab[token] for token in tokens]
print(token_ids)
```

```
[0, 1, 3, 2]
```

If we try to tokenise text with a word not in our vocabulary, we'd get a KeyError. So we need our first example of a special token, `<UNK>`.

```python
vocab['<UNK>'] = len(vocab)
```

Now we can safely encode any text by using `vocab.get(token, vocab['<UNK>'])` instead of `vocab[token]`.

```python
new_text = "Goodbye, world!"
new_tokens = re.findall(r'\w+|[^\w\s]', new_text)
new_ids = [vocab.get(token, vocab['<UNK>']) for token in new_tokens]
print(new_tokens)
print(new_ids)
```

```
['Goodbye', ',', 'world', '!']
[5, 1, 3, 2]
```

- Vocabulary size

## Byte pair encoding (BPE)

Instead of having separate tokens for every word, BPE constructs an efficient set of subword units.

1. Start with individual characters as tokens.
2. Find the most frequent pair of adjacent tokens.
3. Merge that pair into a new token.
4. Repeat until we reach the desired size of our vocabulary.

- Word-level BPE splits on whitespace first, then applies BPE within each word. This is what GPT-2 does.
- Pure character-level BPE treats the entire text as one sequence and applies merges across everything, including spaces.

- Special space markers (like GPT2)
- Byte-level encoding

[Implementation from scratch at this link](https://github.com/HenryDashwood/ai-notes/blob/master/tokenisation/byte_pair_encoding_from_scratch.ipynb)

## Converting tokens to embeddings

How embedding layers function as a lookup operation, retrieving vectors corresponding to token IDs.

## Positional embeddings

- Absolute positional embeddings
  - OpenAI style absolute embeddings that are optimised during training

- Relative positional embeddings

- Rotatary positional embeddings
