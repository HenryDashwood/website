The simplest tokeniser:

1. Splits text into chunks in some rule-based way, e.g. a word-based tokeniser would split on sentences and punctuation.

```python
text = "Hello, world!"
tokens = text.split()
print(tokens)
```

```
['Hello,', 'world!']
```

Python's `split()` method only splits on spaces by default. We can use regular expressions to cover spaces and punctuation that also delimit words.

<details>
<summary>How does the `re.findall(r'\w+|[^\w\s]')` regex work?</summary>

It has two parts separated by `|` (which means "or"):

- `\w+` matches one or more word characters (letters, digits, underscores).
- `[^\w\s]` matches anything that's not a word character and not whitespace. So it matches punctuation.

The `^` inside the brackets `[^...]` means "not", so `[^\w\s]` literally means "not a word character, not whitespace".

---

Both `split()` and `findall()` break text into pieces, but they work differently:

- `split()`: You tell it what to remove (the separators), and it gives you what's left.
- `findall()`: You tell it what to keep (the pattern), and it gives you all matches.

</details>

```python
import re
tokens = re.findall(r'\w+|[^\w\s]', text)
print(tokens)
```

```
['Hello', ',', 'world', '!']
```

2. Maps those chunks to numeric values:

```python
vocab = {token: idx for idx, token in enumerate(set(tokens))}
print(vocab)
```

```
{'Hello': 0, ',': 1, '!': 2, 'world': 3}
```

```python
token_ids = [vocab[token] for token in tokens]
print(token_ids)
```

```
[0, 1, 3, 2]
```

If we try to tokenise text with a word not in our vocabulary, we'd get a KeyError. So we need our first example of a special token, `<UNK>`.

```python
vocab['<UNK>'] = len(vocab)
```

Now we can safely encode any text by using `vocab.get(token, vocab['<UNK>'])` instead of `vocab[token]`.

```python
new_text = "Goodbye, world!"
new_tokens = re.findall(r'\w+|[^\w\s]', new_text)
new_ids = [vocab.get(token, vocab['<UNK>']) for token in new_tokens]
print(new_tokens)
print(new_ids)
```

```
['Goodbye', ',', 'world', '!']
[5, 1, 3, 2]
```

- Vocabulary size

## Byte pair encoding (BPE)

Instead of having separate tokens for every word, BPE constructs an efficient set of subword units.

1. Start with individual characters as tokens.
2. Find the most frequent pair of adjacent tokens.
3. Merge that pair into a new token.
4. Repeat until we reach the desired size of our vocabulary.

- Word-level BPE splits on whitespace first, then applies BPE within each word. This is what GPT-2 does.
- Pure character-level BPE treats the entire text as one sequence and applies merges across everything, including spaces.

- Special space markers (like GPT2)
- Byte-level encoding

[Implementation from scratch at this link](https://github.com/HenryDashwood/ai-notes/blob/master/tokenisation/byte_pair_encoding_from_scratch.ipynb)

```python
class BPE:
    def __init__(self, vocab_size: int):
        self.special_tokens = ["<UNK>", "<PAD>", "<BOS>", "<EOS>"]
        self.vocab: dict[int, bytes] = {i: bytes([i]) for i in range(256)}
        self.merges: dict[tuple[int, int], int] = {}
        self.vocab_size = vocab_size

    def encode(self, text: str) -> list[int]:
        token_bytes: bytes = text.encode(encoding="utf-8")
        tokens: list[int] = list(map(int, token_bytes))
        for pair, idx in self.merges.items():
            tokens = self.merge_pair(tokens, pair, idx)
        return tokens

    def decode(self, tokens: list[int]) -> str:
        token_bytes: bytes = b"".join(self.vocab[token] for token in tokens)
        return token_bytes.decode("utf-8", errors="replace")

    def train(self, text: str):
        tokens = self.get_tokens(text)
        for i in range(256, self.vocab_size):
            pair_counts = self.get_pair_counts(tokens)
            most_common_pair = max(pair_counts, key=lambda p: pair_counts[p])
            tokens = self.merge_pair(tokens, most_common_pair, i)
            self.merges[most_common_pair] = i
            self.vocab[i] = self.vocab[most_common_pair[0]] + self.vocab[most_common_pair[1]]

    def get_tokens(self, text: str) -> list[int]:
        tokens: bytes = text.encode(encoding="utf-8")
        return list(map(int, tokens))

    def get_pair_counts(self, tokens: list[int]) -> dict[tuple[int, int], int]:
        counts: dict[tuple[int, int], int] = {}
        for pair in zip(tokens, tokens[1:]):
            counts[pair] = counts.get(pair, 0) + 1
        return counts

    def merge_pair(self, tokens: list[int], pair: tuple[int, int], idx: int) -> list[int]:
        new_tokens = []
        i = 0
        while i < len(tokens):
            if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:
                new_tokens.append(idx)
                i += 2
            else:
                new_tokens.append(tokens[i])
                i += 1
        return new_tokens
```

### Avoiding merging across accross word boundaries

Paraphrasing [the GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) here.

Consider the word "dog". In a large corpus it appears in many contexts:

- `dog`
- `dog.`
- `dog!`
- `dog?`
- `dog,`
- ` dog`

If we trained a BPE tokenizer like the one above, it might decide to merge `d+o+g+.` into a single token `dog.`, and separately `d+o+g+!` into `dog!`, etc. Each variant would eat up a slot in our fixed vocabulary budget. We'd be wasting indexes on what is essentially the same concept, and the model have to learn that all these variants referred to almosts the same thing.

If we "pre-tokenise" the text using a regex that splits on character categories (e.g. word characters, whitespace, punctuation), and don't allow the tokeniser to merge across these category boundaries, we can avoid this problem. Something like `dog+.` can never be fused into a single token.

We make an exception for the space character. This can merge with other word characters. This is because spaces are so common we get enough benefits from compressing the text that it outweights some word fragmentation.
